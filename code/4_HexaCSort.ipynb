{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "\n",
    "train_df = pd.read_csv(r'./train.csv')\n",
    "test_df = pd.read_csv(r'./train.csv')\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "additional_input_train = pd.read_csv('./sequence_train_dense_output.csv')\n",
    "additional_input_test = pd.read_csv('./sequence_test_dense_output.csv')\n",
    "\n",
    "additional_input_train.drop(additional_input_train.columns[0], axis=1, inplace=True)\n",
    "additional_input_test.drop(additional_input_test.columns[0], axis=1, inplace=True)\n",
    "\n",
    "train_dense = pd.read_csv('./sequence_train_dense_output.csv')\n",
    "test_dense = pd.read_csv('./sequence_test_dense_output.csv')\n",
    "\n",
    "train = pd.concat([additional_input_train, train_dense], axis=1)\n",
    "test = pd.concat([additional_input_test, test_dense], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten,Conv1D\n",
    "from keras.layers import Dropout, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "def HexaCSort(X_train, y_train, X_test, y_test):\n",
    "    inputShape=(328,1)\n",
    "    input = Input(inputShape)\n",
    "    x = Conv1D(32,(3), strides = (1), padding='same')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D((2), padding = \"same\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(32, activation = 'relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(2,activation = 'softmax')(x)\n",
    "    model = Model(inputs = input, outputs = x)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience = 30,restore_best_weights = True)\n",
    "    callbacks_list = [early_stop]\n",
    "    model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                            epochs=500,callbacks=callbacks_list,batch_size = 512, verbose=1)\n",
    "    return model, model_history"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e3fc955140f3775"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle = True, random_state=0)\n",
    "\n",
    "ACC_collecton = []\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "precison_collecton = []\n",
    "F1_collecton = []\n",
    "cv = 1\n",
    "for train_index , test_index in kf.split(y_train):\n",
    "    X_train_CV , X_valid_CV = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "    y_train_CV , y_valid_CV = y_train.iloc[train_index] , y_train.iloc[test_index]\n",
    "\n",
    "    print(X_train_CV.shape)\n",
    "    print(y_train_CV.shape)\n",
    "    print(X_valid_CV.shape)\n",
    "    print(y_valid_CV.shape)\n",
    "\n",
    "    model, model_history = HexaCSort(X_train_CV, y_train_CV, X_valid_CV, y_valid_CV)\n",
    "    model.save(f'./model/model_cv_{cv}', save_format = 'tf')\n",
    "    cv += 1\n",
    "    predicted_class= []\n",
    "    predicted_protability = model.predict(X_valid_CV, batch_size=1)\n",
    "    for i in range(predicted_protability.shape[0]):\n",
    "        index = np.where(predicted_protability[i] == np.amax(predicted_protability[i]))[0][0]\n",
    "        predicted_class.append(index)\n",
    "    predicted_class = np.array(predicted_class)\n",
    "    y_true = y_valid_CV\n",
    "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel()\n",
    "    print('TP, FP, FN, TN:', TP, FP, FN, TN)\n",
    "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "    ACC_collecton.append(ACC)\n",
    "    Sn_collecton.append(TP/(TP+FN))\n",
    "    Sp_collecton.append(TN/(TN+FP))\n",
    "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "    MCC_collecton.append(MCC)\n",
    "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "    precison_collecton.append(TP/(TP+FP))\n",
    "    F1 = (2*TP)/(2*TP+FN+FP)\n",
    "    F1_collecton.append(F1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0c3798858629194"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statistics import mean, stdev\n",
    "print('acc',mean(ACC_collecton),'±',stdev(ACC_collecton))\n",
    "print('bacc',mean(BACC_collecton),'±',stdev(BACC_collecton))\n",
    "print('sn',mean(Sn_collecton),'±',stdev(Sn_collecton))\n",
    "print('sp',mean(Sp_collecton),'±',stdev(Sp_collecton))\n",
    "print('mcc',mean(MCC_collecton),'±',stdev(MCC_collecton))\n",
    "print('precison',mean(precison_collecton),'±',stdev(precison_collecton))\n",
    "print('f1',mean(F1_collecton),'±',stdev(F1_collecton))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d806af1c6ffb55c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_paths = ['./model/model_cv_1',\n",
    "               './model/model_cv_2',\n",
    "               './model/model_cv_3',\n",
    "               './model/model_cv_4',\n",
    "               './model/model_cv_5']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff5534695de761b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ACC_collecton = []\n",
    "BACC_collecton = []\n",
    "Sn_collecton = []\n",
    "Sp_collecton = []\n",
    "MCC_collecton = []\n",
    "precison_collecton = []\n",
    "F1_collecton = []\n",
    "\n",
    "for model_path in model_paths:\n",
    "    pred = []\n",
    "    predicted_class= []\n",
    "    model = keras.models.load_model(model_path)\n",
    "    predictions = model.predict(test, batch_size=1)\n",
    "    pred.append(predictions)\n",
    "    print(pred)\n",
    "    pred = np.array(pred).reshape(284,2)\n",
    "    for i in range(pred.shape[0]):\n",
    "      index = np.where(pred[i] == np.max(pred[i]))[0][0]\n",
    "      predicted_class.append(index)\n",
    "    predicted_class = np.array(predicted_class)\n",
    "    y_true = y_test\n",
    "    print(y_true.shape)\n",
    "    print(predicted_class)\n",
    "    TP, FP, FN, TN = confusion_matrix(y_true, predicted_class).ravel()\n",
    "    print('TP, FP, FN, TN:', TP, FP, FN, TN)\n",
    "    ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "    ACC_collecton.append(ACC)\n",
    "    Sn_collecton.append(TP/(TP+FN))\n",
    "    Sp_collecton.append(TN/(TN+FP))\n",
    "    MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
    "    MCC_collecton.append(MCC)\n",
    "    BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
    "    precison_collecton.append(TP/(TP+FP))\n",
    "    F1 = (2*TP)/(2*TP+FN+FP)\n",
    "    F1_collecton.append(F1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a00bb5b721a5de96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from statistics import stdev\n",
    "import scipy.stats as stats\n",
    "t_acc = 0\n",
    "t_bacc = 0\n",
    "t_sn = 0\n",
    "t_sp = 0\n",
    "t_mcc = 0\n",
    "t_pre = 0\n",
    "t_f1 = 0\n",
    "for i in range(5):\n",
    "    print(f\"{i} -------------------\")\n",
    "    print('ACC: ', ACC_collecton[i])\n",
    "    print('BACC: ', BACC_collecton[i])\n",
    "    print('Sn: ', Sn_collecton[i])\n",
    "    print('Sp: ', Sp_collecton[i])\n",
    "    print('MCC: ', MCC_collecton[i])\n",
    "    print('Precision: ', precison_collecton[i])\n",
    "    print('F1: ', F1_collecton[i])\n",
    "    \n",
    "    t_acc += ACC_collecton[i]\n",
    "    t_bacc += BACC_collecton[i]\n",
    "    t_sn += Sn_collecton[i]\n",
    "    t_sp += Sp_collecton[i]\n",
    "    t_mcc += MCC_collecton[i]\n",
    "    t_pre += precison_collecton[i]\n",
    "    t_f1 += F1_collecton[i]\n",
    "\n",
    "print(\"total ---------------------\")\n",
    "print('ACC: ', t_acc/5, '±', stdev(ACC_collecton))\n",
    "print('BACC: ', t_bacc/5, '±', stdev(BACC_collecton))\n",
    "print('Sn: ', t_sn/5, '±', stdev(Sn_collecton))\n",
    "print('Sp: ', t_sp/5, '±', stdev(Sp_collecton))\n",
    "print('MCC: ', t_mcc/5, '±', stdev(MCC_collecton))\n",
    "print('Precision: ', t_pre/5, '±', stdev(precison_collecton))\n",
    "print('F1: ', t_f1/5, '±', stdev(F1_collecton))\n",
    "\n",
    "mean_accuracy = np.mean(ACC_collecton)\n",
    "\n",
    "confidence_level = 0.95\n",
    "degrees_freedom = len(ACC_collecton) - 1\n",
    "confidence_interval = stats.t.interval(confidence_level, degrees_freedom, mean_accuracy, stats.sem(ACC_collecton))\n",
    "\n",
    "print(mean_accuracy)\n",
    "print(confidence_interval)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a733d7a0a1fa594"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
